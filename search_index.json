[["statistics-perspective.html", "Chapter 7 Statistics perspective 7.1 Statistical models and scientific hypotheses 7.2 Probability", " Chapter 7 Statistics perspective Compared to data science, using statistics has long history in research. The goals could be summarized as: Describe: summarize data in a simplified way that we can understand Decide: make decisions based on data, in the face of uncertainty Predict: make predictions about new situations based on our knowledge of previous situations Lets consider an example where we have data across 100 000 study participants (which we call the sample) regarding their vitamin D levels. We also have the health records available and would be interested to study association between the vitamin levels and disease prevalence, for example. Compared to machine learning, statistics tries to make the data better understandable for a human. Given our study of 100,000 people, one way to describe this data in a simple way is to divide the data into groups, after ordering them in terms of their intake of vitamin D; the first quantile contains the 20% of people with the lowest intake, and the 5th quantile contains the 20% with the highest intake. Now, instead of 100,000 data points, we could plot vitamin D intake level (categories) against cancer-related deaths. In fact, one way to think about statistics is aggregation, i.e. we throw away details of the study participants and make conclusions based on data summaries that consists of a small set of data metrics, or statistics (e.g mean and standard deviation, or reporting quantiles). If we indeed observed a trend between vitamin levels and cancer based on our summary, we might want to proceed to evaluate this relationship further. There is a lot of uncertainty in the data, in any real-world dataset; for example, it would be normal to have some people with high vitamin intake who died of cancer, and, similarly, some people with very low intake who never had cancer. Given the observed variability, we want to decide whether the trend we see in the data is large enough that we wouldnt expect it to occur randomly if there was not truly a relationship (null hypothesis) between vitamin intake and cancer incidence. Statistics provide the tools to make these kinds of decisions. 7.1 Statistical models and scientific hypotheses Below you find a set of steps that we generally go through when we want to use a statistical model to test a scientific hypothesis (decide to accept or reject it): Specify your question of interest Identify or collect the appropriate data Prepare the data for analysis Determine the appropriate model Fit the model to the data Criticize the model to make sure it fits properly Test hypothesis and quantify effect size Choice of model Centuries of statistics research have provides guidelines that can help decide what type of models are suitable for which type of data and questions. For example, a normal distribution fit could be a good model to capture key properties (such as expected value) for continuous valued measurements but not appropriate for discrete data such as counts. One such example is sequencing data! Before, in the 1990s when microarray technology was used to compare gene expression levels in biological samples, researchers could utilize the powerful statistics tools available for continuous (log)-normal data, including linear models. The whole workflow had to be re-invented, i.e. built around a different toolset suitable for discrete data, when sequencing-based count data from RNA-seq became common. The good and bad p-value The idea of following steps 1-7 is very logical. However, it turns out that it is more difficult to publish results when the study concludes that the hypothesis proposed initially should be rejected (negative results). Similarly, expensive experiments have led to design of minimalistic studies where we attempt to make conclusions based on smallest possible sample sizes. This has led to many problems, referred to as p-hacking. So a word of caution is at place here about the pitfalls you should avoid: p-hacking is if you: -Analyze data after every subject, and stop collecting data once p&lt;0.05 -Analyze many different variables, but only report those with p&lt;0.05 -Collect many different experimental conditions, but only report those with p&lt;0.05 -Exclude participants to get p&lt;0.05 -Transform the data to get p&lt;0.05 Any of the above, corrupts the whole scientific process, as no real conclusions can be drawn. 7.2 Probability Probability is the branch of mathematics that deals with chance and uncertainty. Instead of going after some arbitrary p-value threshold, it is much more useful to accept that there is uncertainty in our conclusions, for example by including confidence intervals, and using concepts from probability theory. You might associate probabilities with the calculations you learned in school for experiments such as rolling a dice, where a certain number of possible outcomes are possible. Then you apply the calculation on a certain observed sequence of events (sample). Alternatively, you could be observing passing cars at the intersection and recording those that are electric cars. Many biomedical experiments involve sampling and can be related to the classical probability experiments. For example, a blood sample where we count certain cell types is similar to the electric car counting situation, and mathematically a model (probability distribution) has been proposed that allows you to calculate the probability of observing a certain number of events (e.g. T-lymphocytes) per fixed time/volume (1 ml of blood sample), if we know how many to expect on average. A Poisson model can be used to calculate the probability of a given number of events occurring in a fixed interval of time or space, i.e. it would be appropriate for the situation above. The choice of the model is based on the type of experiment that was performed. From this basic foundation, an interesting path of probability theory emerges - this idea was so revolutionary that it led to two different statistics schools - the frequentists and Bayesians. To get an idea what the difference is, it is first important to understand conditional probabilities, from which the Bayes rule is derived. We will not go into details here but instead ask - Why is this interesting? In simple words, the Bayes rule gives us a mathematical way to update our beliefs on the basis of data . Frequentists interprets probabilities in long-run i.e. in situations where we can sample many times (or indefinately), e.g. coin flip. This is less useful if the event can only happen once. For Bayesians probability is as a degree of belief in a particular proposition. This would be better-suited e.g. to answer at the start of the Covid-19 pandemic in 2019 How likely will we have a vaccine developed during 2020?. The frequencies to calculate the frequentist probability were not available. Based on our prior beliefs (e.g. how long it took historically to develop vaccines) and knowledge (e.g. how many labs worked on it then, how many succeeded, and how many would be trying to develop the drug now), we could propose a Bayesian probability calculation for this situation. Predicting is important concept in statistics which is also encountered in machine learning. In both, the idea is to fit a model to the data collected. It is good to keep in mind that when we want to generalize from the data we already have to some other situation, we assume that the particular sample that we have available is representative of a larger population. Models (and predictions made based on them) can also be updated when new data is collected (Bayes rule). Many of the sophisticated predictive machine learning models have their roots in probability theory. Additional reading: Statistics versus Machine learning "],["sign-up-for-inter-disciplinary-training.html", "Chapter 8 Sign-up for inter-disciplinary training!", " Chapter 8 Sign-up for inter-disciplinary training! In conclusion, because modern biomedical challenges involves biological systems that are very complex, multi-scale and multi-parametric, the use of traditional intuition-based scientific methodologies need to be complemented with additional tools. In this brief introduction, we have mentioned systems-level thinking, network analysis, data science tools, biostatistics and machine learning methods, and computer programming as part of this survival toolkit. These topics are covered in several courses provided in your University, and this knowledge in quantitative science in the broad sense can be useful well beyond biomedicine (e.g., road traffic regulation, Earth dynamics &amp; climate change). Signing-up to such courses will help you develop inter-disciplinary skills early enough in your career, to provide you with high-flexibility to adjust to various scientific environments and anticipate evolutions in the job market. "]]
